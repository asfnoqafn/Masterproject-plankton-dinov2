{"cells":[{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# A simple notebook demonstrating how to extract an attention map from DinoV2 inference (with registers) \n","\n","# Most of the core code was originally published here:\n","#  https://gitlab.com/ziegleto-machine-learning/dino/-/tree/main/\n","\n","# November 11th, 2023 by Lance Legel (lance@3co.ai) from 3co, Inc. (https://3co.ai)\n","\n","\n","# Additional resources:\n","# https://www.kaggle.com/code/stpeteishii/dino-visualize-self-attention-sample\n","# https://gitlab.com/ziegleto-machine-learning/dino/-/tree/main/\n","\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import os\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import warnings\n","warnings.filterwarnings('ignore', category=UserWarning)\n","from torchvision import datasets, transforms\n","import numpy as np\n","from PIL import Image\n","from dinov2.models.vision_transformer import vit_small, vit_base, vit_large\n","from matplotlib.colors import Normalize"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# These are settings for ensuring input images to DinoV2 are properly sized\n","\n","class ResizeAndPad:\n","    def __init__(self, target_size, multiple):\n","        self.target_size = target_size\n","        self.multiple = multiple\n","\n","    def __call__(self, img):\n","        # Resize the image\n","        img = transforms.Resize(self.target_size)(img)\n","\n","        # Calculate padding\n","        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n","        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n","\n","        # Apply padding\n","        img = transforms.Pad((pad_width // 2, pad_height // 2, pad_width - pad_width // 2, pad_height - pad_height // 2))(img)\n","\n","        \n","        return img\n","\n","# image_dimension = 952\n","image_dimension = 150\n","    \n","# This is what DinoV2 sees\n","target_size = (image_dimension, image_dimension)\n","\n","# During inference / testing / deployment, we want to remove data augmentations from the input transform:\n","data_transforms = transforms.Compose([ ResizeAndPad(target_size, 14),\n","                                       transforms.CenterCrop(image_dimension),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize([0.68622917, 0.68622917, 0.68622917], [0.10176649, 0.10176649, 0.10176649]),\n","                                     ]\n","                                     )"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]},{"data":{"text/plain":["DinoVisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n","    (norm): Identity()\n","  )\n","  (blocks): ModuleList(\n","    (0-11): 12 x NestedTensorBlock(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): MemEffAttention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): LayerScale()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): LayerScale()\n","      (drop_path2): Identity()\n","    )\n","  )\n","  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","  (head): Identity()\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["image_size = (image_dimension, image_dimension)\n","output_dir = '.'\n","patch_size = 14\n","# n_register_tokens = 4\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)\n","\n","model = vit_small(\n","        patch_size=14,\n","        img_size=526,\n","        init_values=1.0,\n","        # num_register_tokens=n_register_tokens,\n","        block_chunks=0\n",")\n","\n","model.load_state_dict(torch.load('/home/hgf_mdc/hgf_ysb1444/checkpoints/dinov2_vits14_pretrain.pth'))\n","for p in model.parameters():\n","    p.requires_grad = False\n","model.to(device)\n","model.eval()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"image/jpeg":"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","image/png":"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","text/plain":["<PIL.PngImagePlugin.PngImageFile image mode=L size=164x78>"]},"metadata":{},"output_type":"display_data"}],"source":["# URL of the image\n","# filename = \"/hkfs/work/workspace_haic/scratch/hgf_ysb1444-plankton_ws/plankton-dinov2/images/800px-STS-124_launch_from_a_distance.jpg\"\n","filename = \"/home/hgf_mdc/hgf_ysb1444/data/2007/Tontonia_gracillima/IFCB1_2007_141_111713_00338.png\"\n","\n","\n","original_image = Image.open(open(filename, 'rb'))\n","original_image = original_image.convert('RGB')\n","\n","# Display the image\n","display(original_image)\n","\n","(original_w, original_h) = original_image.size\n","\n","img = data_transforms(original_image)\n","\n","# make the image divisible by the patch size\n","w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n","img = img[:, :w, :h]\n","\n","w_featmap = img.shape[-2] // patch_size\n","h_featmap = img.shape[-1] // patch_size\n","\n","img = img.unsqueeze(0)\n","img = img.to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unexpected exception formatting exception. Falling back to standard exception\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/scratch/ipykernel_857261/1039912113.py\", line 1, in <module>\n","    attention = model.get_last_self_attention(img.to(device))\n","  File \"/hkfs/work/workspace_haic/scratch/hgf_ysb1444-plankton_ws/plankton-dinov2/dinov2/models/vision_transformer.py\", line 381, in get_last_self_attention\n","    x = blk(x)\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/hkfs/work/workspace_haic/scratch/hgf_ysb1444-plankton_ws/plankton-dinov2/dinov2/layers/block.py\", line 294, in forward\n","    return super().forward(x_or_x_list, return_attention)\n","  File \"/hkfs/work/workspace_haic/scratch/hgf_ysb1444-plankton_ws/plankton-dinov2/dinov2/layers/block.py\", line 121, in forward\n","    x = x + attn_residual_func(x)\n","  File \"/hkfs/work/workspace_haic/scratch/hgf_ysb1444-plankton_ws/plankton-dinov2/dinov2/layers/block.py\", line 97, in attn_residual_func\n","    return self.ls1(self.attn(self.norm1(x)))\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/hkfs/work/workspace_haic/scratch/hgf_ysb1444-plankton_ws/plankton-dinov2/dinov2/layers/attention.py\", line 89, in forward\n","    x = memory_efficient_attention(q, k, v, attn_bias=attn_bias)\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/xformers/ops/fmha/__init__.py\", line 196, in memory_efficient_attention\n","    return _memory_efficient_attention(\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/xformers/ops/fmha/__init__.py\", line 294, in _memory_efficient_attention\n","    return _memory_efficient_attention_forward(\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/xformers/ops/fmha/__init__.py\", line 310, in _memory_efficient_attention_forward\n","    op = _dispatch_fw(inp)\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/xformers/ops/fmha/dispatch.py\", line 98, in _dispatch_fw\n","    return _run_priority_list(\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/xformers/ops/fmha/dispatch.py\", line 73, in _run_priority_list\n","    raise NotImplementedError(msg)\n","NotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs:\n","     query       : shape=(1, 101, 6, 64) (torch.float32)\n","     key         : shape=(1, 101, 6, 64) (torch.float32)\n","     value       : shape=(1, 101, 6, 64) (torch.float32)\n","     attn_bias   : <class 'NoneType'>\n","     p           : 0.0\n","`flshattF` is not supported because:\n","    device=cpu (supported: {'cuda'})\n","    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\n","`tritonflashattF` is not supported because:\n","    device=cpu (supported: {'cuda'})\n","    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\n","    Operator wasn't built - see `python -m xformers.info` for more info\n","    triton is not available\n","`cutlassF` is not supported because:\n","    device=cpu (supported: {'cuda'})\n","`smallkF` is not supported because:\n","    max(query.shape[-1] != value.shape[-1]) > 32\n","    unsupported embed per head: 64\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n","    frames.append(self.format_record(record))\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n","    frame_info.lines, Colors, self.has_colors, lvals\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n","    return self._sd.lines\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n","    pieces = self.included_pieces\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n","    pos = scope_pieces.index(self.executing_piece)\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n","    return only(\n","  File \"/home/hgf_mdc/hgf_ysb1444/micromamba/envs/dinov2/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n","    raise NotOneValueFound('Expected one value, found 0')\n","executing.executing.NotOneValueFound: Expected one value, found 0\n"]}],"source":["attention = model.get_last_self_attention(img.to(device))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Attention {}: {}\".format(attention.shape, attention))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["number_of_heads = attention.shape[1]\n","\n","# attention tokens are packed in after the first token; the spatial tokens follow\n","# attention = attention[0, :, 0, 1 + n_register_tokens:].reshape(number_of_heads, -1)\n","attention = attention[0, :, 0, 1:].reshape(number_of_heads, -1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(attention.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# resolution of attention from transformer tokens\n","attention = attention.reshape(number_of_heads, w_featmap, h_featmap)\n","print(attention.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# upscale to higher resolution closer to original image\n","attention = nn.functional.interpolate(attention.unsqueeze(0), scale_factor=patch_size, mode = \"nearest\")[0].cpu()\n","print(attention.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# sum all attention across the 12 different heads, to get one map of attention across entire image\n","attention = torch.sum(attention, dim=0)\n","print(attention.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# interpolate attention map back into original image dimensions\n","attention_of_image = nn.functional.interpolate(attention.unsqueeze(0).unsqueeze(0), size=(original_h, original_w), mode='bilinear', align_corners=False)\n","attention_of_image = attention_of_image.squeeze()\n","print(attention_of_image.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Normalize image_metric to the range [0, 1]\n","image_metric = attention_of_image.numpy()\n","normalized_metric = Normalize(vmin=image_metric.min(), vmax=image_metric.max())(image_metric)\n","\n","# Apply the Reds colormap\n","reds = plt.cm.Reds(normalized_metric)\n","\n","# Create the alpha channel\n","alpha_max_value = 1.00  # Set your max alpha value\n","\n","# Adjust this value as needed to enhance lower values visibility\n","gamma = 0.5  \n","\n","# Apply gamma transformation to enhance lower values\n","enhanced_metric = np.power(normalized_metric, gamma)\n","\n","# Create the alpha channel with enhanced visibility for lower values\n","alpha_channel = enhanced_metric * alpha_max_value\n","\n","# Add the alpha channel to the RGB data\n","rgba_mask = np.zeros((image_metric.shape[0], image_metric.shape[1], 4))\n","rgba_mask[..., :3] = reds[..., :3]  # RGB\n","rgba_mask[..., 3] = alpha_channel  # Alpha\n","\n","# Convert the numpy array to PIL Image\n","rgba_image = Image.fromarray((rgba_mask * 255).astype(np.uint8))\n","\n","# Save the image\n","rgba_image.save('attention_mask.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the attention mask with PIL\n","attention_mask_image = Image.open(\"{}/attention_mask.png\".format(output_dir))\n","\n","# Ensure both images are in the same mode\n","if original_image.mode != 'RGBA':\n","    original_image = original_image.convert('RGBA')\n","\n","# Overlay the second image onto the first image\n","# The second image must be the same size as the first image\n","original_image.paste(attention_mask_image, (0, 0), attention_mask_image)\n","\n","# Save or show the combined image\n","original_image.save('image_with_attention.png')\n","\n","# Or display it\n","display(original_image)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":2,"payload":{"allShortcutsEnabled":false,"blob":{"csv":null,"csvError":null,"dependabotInfo":{"configFilePath":null,"configurationNoticeDismissed":null,"currentUserCanAdminRepo":false,"dismissConfigurationNoticePath":"/settings/dismiss-notice/dependabot_configuration_notice","networkDependabotPath":"/3cology/dinov2_with_attention_extraction/network/updates","repoAlertsPath":"/3cology/dinov2_with_attention_extraction/security/dependabot","repoOwnerIsOrg":true,"repoSecurityAndAnalysisPath":"/3cology/dinov2_with_attention_extraction/settings/security_analysis","showConfigurationBanner":false},"discussionTemplate":null,"displayName":"attention.ipynb","displayUrl":"https://notebooks.githubusercontent.com/view/ipynb?browser=unknown_browser&bypass_fastly=true&color_mode=auto&commit=df7265ce09efa7553a537606565217e42cefea32&device=unknown_device&docs_host=https%3A%2F%2Fdocs.github.com&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f33636f6c6f67792f64696e6f76325f776974685f617474656e74696f6e5f65787472616374696f6e2f646637323635636530396566613735353361353337363036353635323137653432636566656133322f6e6f7465626f6f6b732f617474656e74696f6e2e6970796e62&logged_in=false&nwo=3cology%2Fdinov2_with_attention_extraction&path=notebooks%2Fattention.ipynb&platform=unknown_platform&repository_id=717645547&repository_type=Repository&version=0","headerInfo":{"blobSize":"3.08 MB","deleteInfo":{"deleteTooltip":"You must be signed in to make or propose changes"},"editInfo":{"editTooltip":"You must be signed in to make or propose changes"},"ghDesktopPath":"https://desktop.github.com","gitLfsPath":null,"isCSV":false,"isRichtext":false,"lineInfo":{"truncatedLoc":null,"truncatedSloc":null},"mode":"file","onBranch":true,"shortPath":"892b298","siteNavLoginPath":"/login?return_to=https%3A%2F%2Fgithub.com%2F3cology%2Fdinov2_with_attention_extraction%2Fblob%2Fmain%2Fnotebooks%2Fattention.ipynb","toc":null},"image":false,"isCodeownersFile":null,"isPlain":false,"isValidLegacyIssueTemplate":false,"issueTemplate":null,"issueTemplateHelpUrl":"https://docs.github.com/articles/about-issue-and-pull-request-templates","language":null,"languageID":null,"large":true,"loggedIn":false,"newDiscussionPath":"/3cology/dinov2_with_attention_extraction/discussions/new","newIssuePath":"/3cology/dinov2_with_attention_extraction/issues/new","planSupportInfo":{"repoIsFork":null,"repoOwnedByCurrentUser":null,"requestFullPath":"/3cology/dinov2_with_attention_extraction/blob/main/notebooks/attention.ipynb","showFreeOrgGatedFeatureMessage":null,"showPlanSupportBanner":null,"upgradeDataAttributes":null,"upgradePath":null},"publishBannersInfo":{"dismissActionNoticePath":"/settings/dismiss-notice/publish_action_from_dockerfile","dismissStackNoticePath":"/settings/dismiss-notice/publish_stack_from_file","releasePath":"/3cology/dinov2_with_attention_extraction/releases/new?marketplace=true","showPublishActionBanner":false,"showPublishStackBanner":false},"rawBlobUrl":"https://github.com/3cology/dinov2_with_attention_extraction/raw/main/notebooks/attention.ipynb","rawLines":null,"renderImageOrRaw":true,"renderedFileInfo":{"identityUUID":"6f1674d1-d7fb-47c7-8584-d49475359c7c","renderFileType":"ipynb","size":3228076},"richText":null,"shortPath":null,"stylingDirectives":null,"symbols":{"error":{"code":"invalid_argument","meta":{},"msg":"content required"},"not_analyzed":true,"symbols":[],"timed_out":true},"symbolsEnabled":true,"tabSize":8,"topBannersInfo":{"actionsOnboardingTip":null,"citationHelpUrl":"https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-citation-files","globalPreferredFundingPath":null,"overridingGlobalFundingFile":false,"repoName":"dinov2_with_attention_extraction","repoOwner":"3cology","showDependabotConfigurationBanner":false,"showInvalidCitationWarning":false},"truncated":true,"viewable":false,"workflowRedirectUrl":null},"copilotAccessAllowed":false,"copilotInfo":null,"csrf_tokens":{"/3cology/dinov2_with_attention_extraction/branches":{"post":"RTjwA7gRYjFpypSqXsem1yADHxrMIrbsWQO6FdvgagdlpyF85n__aDHftft61TuJSOFIHLeThq9nxZ-IN_vhbQ"},"/repos/preferences":{"post":"XMHN18l0SzatFtA7kjhEr8gJQECFTSPn5CoYnljwV-uYdtg57ZhoThmHQ4-6sO5PU22r51oe9lyfvF772HrzWw"}},"currentUser":null,"fileTree":{"":{"items":[{"contentType":"directory","name":".github","path":".github"},{"contentType":"directory","name":"dinov2","path":"dinov2"},{"contentType":"directory","name":"notebooks","path":"notebooks"},{"contentType":"directory","name":"scripts","path":"scripts"},{"contentType":"file","name":".gitignore","path":".gitignore"},{"contentType":"file","name":"CODE_OF_CONDUCT.md","path":"CODE_OF_CONDUCT.md"},{"contentType":"file","name":"CONTRIBUTING.md","path":"CONTRIBUTING.md"},{"contentType":"file","name":"LICENSE","path":"LICENSE"},{"contentType":"file","name":"MODEL_CARD.md","path":"MODEL_CARD.md"},{"contentType":"file","name":"README.md","path":"README.md"},{"contentType":"file","name":"conda-extras.yaml","path":"conda-extras.yaml"},{"contentType":"file","name":"conda.yaml","path":"conda.yaml"},{"contentType":"file","name":"hubconf.py","path":"hubconf.py"},{"contentType":"file","name":"pyproject.toml","path":"pyproject.toml"},{"contentType":"file","name":"requirements-dev.txt","path":"requirements-dev.txt"},{"contentType":"file","name":"requirements-extras.txt","path":"requirements-extras.txt"},{"contentType":"file","name":"requirements.txt","path":"requirements.txt"},{"contentType":"file","name":"setup.cfg","path":"setup.cfg"},{"contentType":"file","name":"setup.py","path":"setup.py"}],"totalCount":19},"notebooks":{"items":[{"contentType":"file","name":"attention.ipynb","path":"notebooks/attention.ipynb"},{"contentType":"file","name":"depth_estimation.ipynb","path":"notebooks/depth_estimation.ipynb"},{"contentType":"file","name":"semantic_segmentation.ipynb","path":"notebooks/semantic_segmentation.ipynb"}],"totalCount":3}},"fileTreeProcessingTime":4.092453,"foldersToFetch":[],"path":"notebooks/attention.ipynb","reducedMotionEnabled":null,"refInfo":{"canEdit":false,"currentOid":"df7265ce09efa7553a537606565217e42cefea32","listCacheKey":"v0:1699765488.043963","name":"main","refType":"branch"},"repo":{"createdAt":"2023-11-12T05:04:47.000Z","currentUserCanPush":false,"defaultBranch":"main","id":717645547,"isEmpty":false,"isFork":true,"isOrgOwned":true,"name":"dinov2_with_attention_extraction","ownerAvatar":"https://avatars.githubusercontent.com/u/36769967?v=4","ownerLogin":"3cology","private":false,"public":true},"symbolsExpanded":false,"treeExpanded":true},"title":"dinov2_with_attention_extraction/notebooks/attention.ipynb at main · 3cology/dinov2_with_attention_extraction"}
